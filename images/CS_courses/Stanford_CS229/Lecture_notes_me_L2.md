<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [Stanford CS229: Lecture 2](#stanford-cs229-lecture-2)
	- [Notation in this course](#notation-in-this-course)
	- [Regression:](#regression)

<!-- /TOC -->


# Stanford CS229: Lecture 2

## Notation in this course
- $n$: # features
- $m$: # training examples
- $x$: "input" variables/features
- $y$: "output" variables/"target" variables
- $(x,y)$: training examples
- $i^{th}$ training example: $(x^{(i)},y^{(i)})$

## Regression:
- Training set -> Learning algorithm -> hypothesis $h$
- Then $h$ maps a new input to a predicting target (hypothesis is just a term used for historical reason)
- Gradient descent
- Stochastic gradient descent
- Least square solution
